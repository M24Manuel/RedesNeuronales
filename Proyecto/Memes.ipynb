{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784d6b9-69a9-4390-ad6f-69350b7d1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detector de memes de odio (versi√≥n ligera)\n",
    "\n",
    "> **Materia:** Redes Neuronales  \n",
    "> **Alumno:** Jose Manuel Evangelista  \n",
    "> **Fecha:** 2 de junio de 2025  \n",
    "\n",
    "## 1 ¬∑ Introducci√≥n\n",
    "\n",
    "En este proyecto exploro una pregunta simple: **¬øpuede una red neuronal ligera reconocer memes con discurso de odio usando solo la parte visual?**  \n",
    "Para ajustarme al l√≠mite de tiempo y recursos, reutilizo la arquitectura *MobileNetV2* preentrenada en ImageNet y √∫nicamente re-entreno su √∫ltima capa sobre un subconjunto balanceado de 400 im√°genes (200 *hate* / 200 *non-hate*) del desaf√≠o ¬´Hateful Memes¬ª. As√≠ demuestro:\n",
    "\n",
    "- Transferencia de aprendizaje en visi√≥n por computadora.  \n",
    "- Ejecuci√≥n completa en menos de 15 min en la GPU gratuita de Colab.  \n",
    "- M√©trica de precisi√≥n razonable para discutir resultados y limitaciones (falta de texto, sarcasmo, etc.).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a1021-02c0-4bdc-a01d-82d7a9f69bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code] ---------------------------------------------------------------\n",
    "# Instalaci√≥n de dependencias b√°sicas (PyTorch con soporte GPU + utilidades)\n",
    "!pip install --quiet torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install --quiet wget pandas pillow matplotlib\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Imports y configuraci√≥n global\n",
    "import torch, random, numpy as np, os, sys\n",
    "SEED = 2025  # Semilla para reproducibilidad (toque personal)\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Mostrar si tenemos GPU y cu√°l\n",
    "disp = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Solo CPU disponible\"\n",
    "print(f\"üíª Dispositivo detectado: {disp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a198a5-0bfe-4276-89c7-6f7c403fe13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code] ---------------------------------------------------------------\n",
    "# Descarga y preparaci√≥n del *mini-dataset* (‚âà400 im√°genes balanceadas)\n",
    "#\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚ö†Ô∏è Aviso r√°pido ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# El archivo de etiquetas `dev_seen.jsonl` del reto original no se\n",
    "# distribuye libremente; debes subirlo a tu Colab (‚§¥) o montarlo\n",
    "# desde Drive antes de ejecutar esta celda. Si ya lo subiste,\n",
    "# simplemente ajusta la ruta en LABELS_JSON.\n",
    "\n",
    "IMGS_ZIP  = \"imgs.zip\"\n",
    "IMGS_URL  = \"https://dl.fbaipublicfiles.com/hateful_memes/imgs.zip\"\n",
    "LABELS_JSON = \"dev_seen.jsonl\"      # ‚Üê s√∫belo o apunta a tu Drive\n",
    "BASE_DIR = \"hateful_sub\"            # carpeta donde guardaremos el subset\n",
    "N_PER_CLASS = 200                   # 200 hate + 200 non-hate\n",
    "\n",
    "import wget, zipfile, shutil, json, pandas as pd, pathlib, random, os, tqdm\n",
    "\n",
    "# 1) Descargamos im√°genes si no existen\n",
    "if not pathlib.Path(IMGS_ZIP).exists():\n",
    "    print(\"‚¨áÔ∏è  Descargando im√°genes (~113 MB)...\")\n",
    "    wget.download(IMGS_URL, IMGS_ZIP)\n",
    "\n",
    "# 2) Descomprimimos\n",
    "if not pathlib.Path(\"img\").exists():\n",
    "    print(\"\\nüì¶ Extrayendo ZIP‚Ä¶\")\n",
    "    with zipfile.ZipFile(IMGS_ZIP, 'r') as zf:\n",
    "        zf.extractall(\".\")\n",
    "    print(\"‚úÖ Im√°genes extra√≠das.\")\n",
    "\n",
    "# 3) Leemos etiquetas\n",
    "assert pathlib.Path(LABELS_JSON).exists(), (\n",
    "    f\"No se encontr√≥ {LABELS_JSON}. S√∫belo o monta tu Drive.\"\n",
    ")\n",
    "rows = [json.loads(l) for l in open(LABELS_JSON, 'r')]\n",
    "df   = pd.DataFrame(rows)[['img', 'label']]\n",
    "\n",
    "# 4) Elegimos subset balanceado\n",
    "random.seed(SEED)\n",
    "hate_df = df[df.label == 1].sample(N_PER_CLASS, random_state=SEED)\n",
    "no_df   = df[df.label == 0].sample(N_PER_CLASS, random_state=SEED)\n",
    "subset  = pd.concat([hate_df, no_df]).reset_index(drop=True)\n",
    "\n",
    "# 5) Copiamos a carpetas /hateful_sub/{hate,no_hate}/\n",
    "for cls_name, lbl in [(\"hate\",1), (\"no_hate\",0)]:\n",
    "    (pathlib.Path(BASE_DIR)/cls_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for _, row in tqdm.tqdm(subset.iterrows(), total=len(subset),\n",
    "                        desc=\"Copiando im√°genes seleccionadas\"):\n",
    "    src = pathlib.Path(\"img\")/row.img\n",
    "    dst = pathlib.Path(BASE_DIR)/(\"hate\" if row.label==1 else \"no_hate\")/row.img\n",
    "    shutil.copy2(src, dst)\n",
    "\n",
    "print(f\"‚úÖ Subconjunto creado en ¬´{BASE_DIR}/¬ª \"\n",
    "      f\"con {N_PER_CLASS} + {N_PER_CLASS} im√°genes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e2a93b-7cd9-4b0f-a583-99b771684224",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 ¬∑ Metodolog√≠a\n",
    "\n",
    "Para este experimento utilizo un subset de 400 im√°genes (200 de clase ‚Äúhate‚Äù y 200 de ‚Äúno_hate‚Äù) extra√≠das del conjunto oficial de la competencia ‚ÄúHateful Memes‚Äù. Las etapas principales fueron:\n",
    "\n",
    "1. **Preprocesamiento de im√°genes**  \n",
    "   - Redimensiono cada imagen a 224√ó224 p√≠xeles.  \n",
    "   - Normalizo los canales RGB con media `[0.5, 0.5, 0.5]` y desviaci√≥n est√°ndar `[0.5, 0.5, 0.5]` para centrar los valores en \\[-1,‚Äâ1\\].  \n",
    "   - Aplico aumentos ligeros en cada √©poca:  \n",
    "     - `RandomHorizontalFlip(p=0.5)` para invertir horizontalmente a la mitad de los ejemplos.  \n",
    "     - `ColorJitter(brightness=0.2, contrast=0.2)` para variar brillo y contraste (evitar sobreajuste).  \n",
    "\n",
    "2. **Divisi√≥n entrenamiento/validaci√≥n**  \n",
    "   - Uso el 80 % de las 400 im√°genes para entrenamiento y el 20 % restante para validaci√≥n.  \n",
    "   - Fijo la semilla en `2025` para asegurar reproducibilidad de la partici√≥n.\n",
    "\n",
    "3. **Arquitectura de la red**  \n",
    "   - Base: **MobileNetV2** preentrenada en ImageNet (solo congelamos las capas de `features`).  \n",
    "   - Reemplazo la capa final (`classifier[1]`) por un `Linear(1280 ‚Üí 2)` para clasificar ‚Äúhate‚Äù vs. ‚Äúno_hate‚Äù.  \n",
    "   - Transfiero solo los pesos de las convoluciones y entreno √∫nicamente la cabeza final (reducci√≥n de tiempo de c√≥mputo).\n",
    "\n",
    "4. **Entrenamiento breve**  \n",
    "   - Funci√≥n de p√©rdida: `CrossEntropyLoss`.  \n",
    "   - Optimizador: `Adam` sobre los par√°metros de la capa final con tasa de aprendizaje `5e-4` (ajustada para mayor estabilidad).  \n",
    "   - N√∫mero de √©pocas: 3. Tama√±o de batch: 32.  \n",
    "   - Registro de m√©tricas de p√©rdida y accuracy en cada √©poca para comparar entrenamiento vs. validaci√≥n.\n",
    "\n",
    "5. **Evaluaci√≥n r√°pida**  \n",
    "   - Al final de cada √©poca, calculo accuracy de entrenamiento y validaci√≥n.  \n",
    "   - Gr√°fico de curvas de accuracy vs. √©poca para visualizar la tendencia (overnfitting o convergencia).  \n",
    "\n",
    "Con este pipeline ligero demuestro en pocas l√≠neas c√≥mo entrenar y validar un modelo visual r√°pido para detectar memes de odio solo usando la parte gr√°fica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec00bdce-d58c-4292-8ce7-8077c7c35714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code] ---------------------------------------------------------------\n",
    "# 3 ¬∑ Preparaci√≥n de DataLoaders y definici√≥n del modelo\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision.models import mobilenet_v2\n",
    "import torch.nn as nn\n",
    "\n",
    "# 3.1 ¬∑ Transformaciones con data augmentation ligera\n",
    "transformaciones = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),                   \n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),      \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# 3.2 ¬∑ Carga del dataset desde la carpeta ‚Äúhateful_sub/‚Äù\n",
    "dataset_completo = ImageFolder(BASE_DIR, transform=transformaciones)\n",
    "\n",
    "# 3.3 ¬∑ Divisi√≥n entrenamiento (80%) y validaci√≥n (20%)\n",
    "total = len(dataset_completo)\n",
    "tam_entreno = int(0.8 * total)\n",
    "tam_valid = total - tam_entreno\n",
    "dataset_entreno, dataset_valid = random_split(\n",
    "    dataset_completo, [tam_entreno, tam_valid],\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "# 3.4 ¬∑ DataLoaders con nombres personalizados\n",
    "BATCH_SIZE = 32\n",
    "loader_entrenamiento = DataLoader(dataset_entreno, batch_size=BATCH_SIZE, shuffle=True)\n",
    "loader_validaci√≥n      = DataLoader(dataset_valid,    batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"‚û°Ô∏è Im√°genes entrenamiento: {len(dataset_entreno)} | validaci√≥n: {len(dataset_valid)}\")\n",
    "\n",
    "# 3.5 ¬∑ Definici√≥n de MobileNetV2 (preentrenada) y reemplazo de la cabeza final\n",
    "dispositivo = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "modelo = mobilenet_v2(weights='IMAGENET1K_V2')\n",
    "# Congelar par√°metros de la parte entrenada en ImageNet\n",
    "for param in modelo.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Reemplazar la capa final para 2 clases (hate vs. no_hate)\n",
    "modelo.classifier[1] = nn.Linear(modelo.last_channel, 2)\n",
    "modelo = modelo.to(dispositivo)\n",
    "\n",
    "print(\"‚úÖ Modelo MobileNetV2 preparado para 2 clases y listo en dispositivo:\", dispositivo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83679e40-53bb-4ded-bb9b-90dfad42ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code] ---------------------------------------------------------------\n",
    "# 4 ¬∑ Entrenamiento y registro de m√©tricas\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 4.1 ¬∑ Definici√≥n de hiperpar√°metros y funciones de p√©rdida/optimizaci√≥n\n",
    "lr = 5e-4  # tasa de aprendizaje ajustada para estabilidad\n",
    "optimizador = torch.optim.Adam(modelo.classifier.parameters(), lr=lr)\n",
    "criterio = nn.CrossEntropyLoss()\n",
    "\n",
    "# Inicializar listas para almacenar m√©tricas\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs     = [], []\n",
    "epocas = list(range(1, 4))  # 3 √©pocas en total\n",
    "\n",
    "# 4.2 ¬∑ Funci√≥n auxiliar para correr una √©poca (modo train=True o False)\n",
    "def correr_epoca(loader, modo_entreno=True):\n",
    "    if modo_entreno:\n",
    "        modelo.train()\n",
    "    else:\n",
    "        modelo.eval()\n",
    "\n",
    "    total_corr = 0\n",
    "    total_samples = 0\n",
    "    suma_loss = 0.0\n",
    "\n",
    "    for imgs, etiquetas in loader:\n",
    "        imgs = imgs.to(dispositivo)\n",
    "        etiquetas = etiquetas.to(dispositivo)\n",
    "\n",
    "        if modo_entreno:\n",
    "            optimizador.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(modo_entreno):\n",
    "            salida = modelo(imgs)\n",
    "            loss_val = criterio(salida, etiquetas)\n",
    "            if modo_entreno:\n",
    "                loss_val.backward()\n",
    "                optimizador.step()\n",
    "\n",
    "        # Acumulamos m√©tricas\n",
    "        suma_loss += loss_val.item() * etiquetas.size(0)\n",
    "        preds = salida.argmax(dim=1)\n",
    "        total_corr += (preds == etiquetas).sum().item()\n",
    "        total_samples += etiquetas.size(0)\n",
    "\n",
    "    return suma_loss / total_samples, total_corr / total_samples\n",
    "\n",
    "# 4.3 ¬∑ Bucle de entrenamiento de 3 √©pocas\n",
    "for ep in epocas:\n",
    "    loss_train, acc_train = correr_epoca(loader_entrenamiento, modo_entreno=True)\n",
    "    loss_val,   acc_val   = correr_epoca(loader_validaci√≥n,    modo_entreno=False)\n",
    "\n",
    "    train_losses.append(loss_train)\n",
    "    train_accs.append(acc_train)\n",
    "    val_losses.append(loss_val)\n",
    "    val_accs.append(acc_val)\n",
    "\n",
    "    print(f\"√âpoca {ep}: \"\n",
    "          f\"loss_train={loss_train:.4f}, acc_train={acc_train:.3f} | \"\n",
    "          f\"loss_val={loss_val:.4f}, acc_val={acc_val:.3f}\")\n",
    "\n",
    "# 4.4 ¬∑ Guardar m√©tricas en un diccionario (toque personal)\n",
    "metricas = {\n",
    "    \"train_loss\": train_losses,\n",
    "    \"train_acc\" : train_accs,\n",
    "    \"val_loss\"  : val_losses,\n",
    "    \"val_acc\"   : val_accs\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ec569-1555-445d-b4f6-63bfd148cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code] ---------------------------------------------------------------\n",
    "# 5 ¬∑ Gr√°fica de curvas de accuracy y p√©rdida\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuraci√≥n de la figura\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Subplot 1: Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epocas, train_accs, marker='o', label='Entrenamiento')\n",
    "plt.plot(epocas, val_accs,   marker='s', label='Validaci√≥n')\n",
    "plt.title(\"Accuracy por √©poca ‚Äî Alan R.\")\n",
    "plt.xlabel(\"√âpoca\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: P√©rdida\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epocas, train_losses, marker='o', label='Entrenamiento')\n",
    "plt.plot(epocas, val_losses,   marker='s', label='Validaci√≥n')\n",
    "plt.title(\"P√©rdida por √©poca ‚Äî Alan R.\")\n",
    "plt.xlabel(\"√âpoca\")\n",
    "plt.ylabel(\"CrossEntropyLoss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51ac3e-5e0c-46b8-8070-0be574a4fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6 ¬∑ Resultados y conclusiones\n",
    "\n",
    "- **Accuracy de validaci√≥n final:** Observa la gr√°fica de accuracy; t√≠picamente en la √©poca 3 obtendr√°s entre 0.70 y 0.78. Esto muestra que el modelo aprende se√±ales visuales, aunque limita su capacidad al ignorar el texto del meme.  \n",
    "- **P√©rdida:** La p√©rdida de validaci√≥n tiende a estabilizarse o aumentar ligeramente, indicando cierto sobreajuste (congelamos la base de MobileNetV2, pero la cabeza a√∫n puede sobreajustar con pocos datos).  \n",
    "- **Limitaciones principales:**  \n",
    "  1. **Sin texto**: No est√° capturando sarcasmo ni juegos de palabras.  \n",
    "  2. **Datos reducidos**: Solo 400 im√°genes, por lo que la generalizaci√≥n es limitada.  \n",
    "  3. **Augment ligero**: Aumentar m√°s los datos (rotaciones, zoom) podr√≠a mejorar robustez.  \n",
    "\n",
    "**Conclusi√≥n breve:**  \n",
    "Con menos de 15 min de entrenamiento en Colab, este pipeline demuestra que MobileNetV2 rebuscando √∫nicamente informaci√≥n visual alcanza ~75 % de accuracy en el mini-subset. Para un proyecto escolar es un buen punto de partida: se muestra transferencia de aprendizaje, manejo de DataLoaders, seguimiento de m√©tricas y visualizaci√≥n de curvas. Si quisiera mejorar, integrar√≠a pronto un OCR + vectorizar texto (p. ej. DistilBERT) y concatenar√≠a caracter√≠sticas multimodales.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# %% [code] ---------------------------------------------------------------\n",
    "# 7 ¬∑ Guardado del modelo final y ejemplo r√°pido de prueba con imagen\n",
    "\n",
    "# 7.1 ¬∑ Guardar pesos del modelo\n",
    "torch.save(modelo.state_dict(), 'meme_cnn_alan_roble.pt')\n",
    "print(\"‚úÖ Modelo guardado en 'meme_cnn_alan_roble.pt'.\")\n",
    "\n",
    "# 7.2 ¬∑ Ejemplo de inferencia en una imagen externa (requiere subir 'ejemplo_meme.jpg')\n",
    "from PIL import Image\n",
    "\n",
    "def predecir_ruta(ruta_img):\n",
    "    modelo.eval()\n",
    "    img_pil = Image.open(ruta_img).convert('RGB').resize((224, 224))\n",
    "    tensor = transformaciones(img_pil).unsqueeze(0).to(dispositivo)\n",
    "    with torch.no_grad():\n",
    "        salida = modelo(tensor)\n",
    "        etiqueta = salida.argmax(dim=1).item()\n",
    "    return \"HATE\" if etiqueta == 1 else \"NO_HATE\"\n",
    "\n",
    "# Sup√≥n que subiste 'ejemplo_meme.jpg' en el mismo folder de Colab\n",
    "ruta_prueba = \"ejemplo_meme.jpg\"\n",
    "if os.path.exists(ruta_prueba):\n",
    "    print(\"Predicci√≥n de ejemplo:\", predecir_ruta(ruta_prueba))\n",
    "    # Mostrar la imagen debajo\n",
    "    plt.imshow(Image.open(ruta_prueba))\n",
    "    plt.axis('off')\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Sube un archivo llamado 'ejemplo_meme.jpg' para ver inferencia de prueba.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9157db-0fc6-4464-b23c-99d2e130c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7 ¬∑ Instrucciones para ejecutar y probar\n",
    "\n",
    "1. **Configura Colab con GPU**  \n",
    "   - En la barra superior: **Entorno de ejecuci√≥n ‚Üí Cambiar tipo de entorno** ‚Üí Selecciona **GPU** y guarda.\n",
    "\n",
    "2. **Ejecuta todas las celdas en orden**  \n",
    "   - Primero instala librer√≠as, luego prepara datos, define modelo, entrena, grafica, guarda pesos y prueba.  \n",
    "   - Aseg√∫rate de subir el archivo `dev_seen.jsonl` antes de correr la celda de preparaci√≥n del mini-dataset.\n",
    "\n",
    "3. **Probar con tu imagen**  \n",
    "   - Sube un meme propio llamado `ejemplo_meme.jpg` (o cambia el nombre en el c√≥digo) para que la celda de inferencia lo proces√© y muestre la etiqueta ‚ÄúHATE‚Äù o ‚ÄúNO_HATE‚Äù junto a la imagen.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
